import os
import re
import math
import chardet
from collections import Counter

class MailClassifierModel:
    # Ham –±–æ–ª–æ–Ω spam —Ñ–∞–π–ª—É—É–¥—ã–≥ –∞–≥—É—É–ª—Å–∞–Ω —Ñ–æ–ª–¥–µ—Ä–∏–π–Ω –∑–∞–º—ã–≥ –±–∞–π–≥—É—É–ª–∞–≥—á —Ä—É—É –¥–∞–º–∂—É—É–ª–∂ –º–æ–¥–µ–ª—ã–≥ —Å—É—Ä–≥–∞–Ω–∞
    def __init__(self, folder_path_ham, folder_path_spam):
        print("üìò –°—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
        self.folder_path_ham = folder_path_ham
        self.folder_path_spam = folder_path_spam

        # Ham, spam —Ç—É—Å –±“Ø—Ä–¥ –Ω—å unigram-—ã–≥ “Ø“Ø—Å–≥—ç–Ω—ç (Counter –æ–±—ä–µ–∫—Ç –±—É—Ü–∞–∞–Ω–∞)
        self.ham_unigram = self.createUnigram(folder_path_ham)
        self.spam_unigram = self.createUnigram(folder_path_spam)

        # Unigram-—É—É–¥—ã–≥ —Ç–µ–∫—Å—Ç —Ñ–∞–π–ª –¥–æ—Ç–æ—Ä —Ö–∞–¥–≥–∞–ª–Ω–∞
        self.saveUnigram(self.ham_unigram, "ham_unigram.txt")
        self.saveUnigram(self.spam_unigram, "spam_unigram.txt")

        # –ú–æ–¥–µ–ª—ã–≥ —Å—É—Ä–≥–∞–Ω–∞
        self.data = {}
        self.train()

        # “Æ—Ä –¥“Ø–Ω–≥ —Ö—ç–≤–ª—ç–Ω—ç
        self.printResult()

    # –§–∞–π–ª—ã–≥ —É–Ω—à—É—É–¥ —Ç–µ–∫—Å—Ç—ã–≥ —Ü—ç–≤—ç—Ä–ª—ç–Ω—ç
    def getCleanWords(self, file_path):
        try:
            # –§–∞–π–ª—ã–≥ –±–∞–π—Ç–∞–∞—Ä –Ω—å —É–Ω—à–∏–Ω–∞
            with open(file_path, "rb") as f:
                raw_data = f.read()
            # Encoding-—ã–≥ –Ω—å —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–Ω–æ
            result = chardet.detect(raw_data)
            encoding = result["encoding"] or "utf-8"
            # –¢–æ–¥–æ—Ä—Ö–æ–π–ª—Å–æ–Ω encoding-–≥ –∞—à–∏–≥–ª–∞–∂ decode —Ö–∏–π–Ω—ç
            text = raw_data.decode(encoding, errors="ignore")

            # Regular expression –∞—à–∏–≥–ª–∞–∂ —Ç–µ–∫—Å—Ç—ã–≥ —Ü—ç–≤—ç—Ä–ª—ç–Ω—ç
            cleaned = re.sub(r'[^a-zA-ZÍ∞Ä-Ìû£\s]', '', text).lower()
            return cleaned
        
        # –§–∞–π–ª —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–≤–∞–ª –º—ç–¥—ç–≥–¥—ç–Ω—ç
        except Exception as e:
            print(f"‚ö†Ô∏è Could not read {file_path}: {e}")
            return ""

    # –§–∞–π–ª—ã–Ω –∑–∞–º—ã–≥ –¥–∞–º–∂—É—É–ª–∂ ”©–≥”©”©–¥ —É–Ω—à–∞–∞–¥ Unigram-–≥ “Ø“Ø—Å–≥—ç–Ω—ç
    def createUnigram(self, folder_path):
        # Counter –æ–±—ä–µ–∫—Ç (dictionary) –∞—à–∏–≥–ª–∞–∂ “Ø–≥–Ω–∏–π —Ç–æ–æ–≥ —Ö–∞–¥–≥–∞–ª–Ω–∞
        unigram = Counter()

        # –¢—É—Ö–∞–π–Ω —Ñ–æ–ª–¥–µ—Ä –¥–æ—Ç–æ—Ä—Ö —Ñ–∞–π–ª—ã–Ω —Ç–æ–æ–≥–æ–æ—Ä –¥–∞–≤—Ç–∞–Ω–∞
        for filename in os.listdir(folder_path):
            # –§–∞–π–ª –Ω—å —Ç–µ–∫—Å—Ç –±–∏—à ”©—Ä–≥”©—Ç–≥”©–ª—Ç—ç–π –±–æ–ª –∞–ª–≥–∞—Å–Ω–∞
            if not filename.endswith(".txt"):
                continue
            # –§–∞–π–ª—ã–Ω –Ω—ç—Ä–∏–π–≥ –±“Ø—Ç—ç—ç–Ω—ç
            file_path = os.path.join(folder_path, filename)
            # –¶—ç–≤—ç—Ä–ª—ç—Å—ç–Ω —Ç–µ–∫—Å—Ç—ã–≥ –∞–≤–Ω–∞
            cleaned = self.getCleanWords(file_path)
            # –¢–µ–∫—Å—Ç –¥–æ—Ç–æ—Ä—Ö “Ø–≥–Ω“Ø“Ø–¥–∏–π–≥ —Å–∞–ª–≥–∞–Ω–∞
            words = cleaned.split()
            # Unigram —Ä—É—É –æ—Ä—É—É–ª–Ω–∞
            unigram.update(words)
        return unigram

    # Unigram-–≥ —Ç–µ–∫—Å—Ç —Ñ–∞–π–ª —Ä—É—É —Ö–∞–¥–≥–∞–ª–Ω–∞
    def saveUnigram(self, unigram, filename):
        with open(filename, "w", encoding="utf-8") as f:
            for word, count in unigram.most_common():
                f.write(f"{word} {count}\n")
        print(f"‚úÖ {filename}-–≥ —Ö–∞–¥–≥–∞–ª–ª–∞–∞")

    # –ú–æ–¥–µ–ª—ã–≥ —Å—É—Ä–≥–∞–Ω–∞
    def train(self):
        # –Ø–ª–≥–∞–∞—Ç–∞–π “Ø–≥–∏–π–Ω –æ–ª–æ–Ω–ª–æ–≥
        vocab = set(self.ham_unigram.keys()) | set(self.spam_unigram.keys())
        # –Ø–ª–≥–∞–∞—Ç–∞–π “Ø–≥–∏–π–Ω —Ç–æ–æ
        vocab_size = len(vocab)

        # Unigram —Ç—É—Å –±“Ø—Ä–∏–π–Ω –Ω–∏–π—Ç “Ø–≥–∏–π–Ω —Ç–æ–æ
        total_ham_words = sum(self.ham_unigram.values())
        total_spam_words = sum(self.spam_unigram.values())

        # Ham –±–æ–ª–æ–Ω spam —Ñ–∞–π–ª—ã–Ω —Ç–æ–æ
        n_spam_files = len(os.listdir(self.folder_path_spam))
        n_ham_files = len(os.listdir(self.folder_path_ham))
        total_files = n_spam_files + n_ham_files

        # Ham –±–æ–ª–æ–Ω spam-–Ω prior –º–∞–≥–∞–¥–ª–∞–ª
        p_spam = n_spam_files / total_files
        p_ham = n_ham_files / total_files

        # –î—ç—ç—Ä—Ö —Ç–æ–æ—Ü–æ–æ–ª–ª—É—É–¥—ã–≥ —à–∏–Ω–∂–∏–π–Ω —É—Ç–≥–∞–¥ —Ö–∞–¥–≥–∞–ª–Ω–∞
        self.data = {
            "vocab": vocab,
            "vocab_size": vocab_size,
            "ham_total": total_ham_words,
            "spam_total": total_spam_words,
            "ham_counts": self.ham_unigram,
            "spam_counts": self.spam_unigram,
            "p_spam": p_spam,
            "p_ham": p_ham,
        }

        print("‚úÖ –°—É—Ä–≥–∞–ª—Ç –∞–º–∂–∏–ª—Ç—Ç–∞–π.")

    # 1 —Ñ–∞–π–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ –≥–∞—Ä–≥–∞–Ω–∞
    def predict(self, file_path):
        # –§–∞–π–ª –¥–æ—Ç–æ—Ä—Ö “Ø–≥–∏–π–Ω –∂–∞–≥—Å–∞–∞–ª—Ç—ã–≥ –≥–∞—Ä–≥–∞–Ω–∞
        cleaned = self.getCleanWords(file_path)
        words = cleaned.split()

        # “Æ–≥ –±–∞–π—Ö–≥“Ø–π –±–æ–ª –∞–ª–¥–∞–∞ –∑–∞–∞–Ω–∞
        if not words:
            print("‚ö†Ô∏è –§–∞–π–ª—ã–≥ —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞!")
            return "unknown"
        # Laplace smoothing
        a = 1
        # Prior –º–∞–≥–∞–¥–ª–∞–ª—ã–Ω –ª–æ–≥–∞—Ä–∏—Ñ–º
        log_prob_ham = math.log(self.data["p_ham"])
        log_prob_spam = math.log(self.data["p_spam"])

        # “Æ–≥ –±–æ–ª–≥–æ–Ω—ã –º–∞–≥–∞–¥–ª–∞–ª—ã–Ω –ª–æ–≥–∞—Ä–∏—Ñ–º—ã–≥ —Ç–æ–æ—Ü–æ–∂ –Ω—ç–º–Ω—ç
        for w in words:
            p_w_ham = (self.data["ham_counts"][w] + a) / (
                self.data["ham_total"] + a * self.data["vocab_size"]
            )
            p_w_spam = (self.data["spam_counts"][w] + a) / (
                self.data["spam_total"] + a * self.data["vocab_size"]
            )
            log_prob_ham += math.log(p_w_ham)
            log_prob_spam += math.log(p_w_spam)

        # –ê–ª—å –º–∞–≥–∞–¥–ª–∞–ª –Ω—å –∏—Ö –±–∞–π–≥–∞–∞–≥–∞–∞—Ä –∞–Ω–≥–∏–ª–∞–≥–¥–∞–Ω–∞
        return "spam" if log_prob_spam > log_prob_ham else "ham"

    # –ë“Ø—Ç—ç–Ω —Ç–µ—Å—Ç —è–≤—É—É–ª–Ω–∞
    def evaluate(self, dev_folder_path, verbose=True):
        correct = 0
        total = 0

        correct_ham = 0
        correct_spam = 0
        total_ham = 0
        total_spam = 0

        # Ham ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä —Ç—É—Ä—à–∏–Ω–∞
        ham_folder = os.path.join(dev_folder_path, "ham")
        for filename in os.listdir(ham_folder):
            if not filename.endswith(".txt"):
                continue
            file_path = os.path.join(ham_folder, filename)
            prediction = self.predict(file_path)
            if prediction == "ham":
                correct += 1
                correct_ham += 1
            total_ham += 1
            total += 1
            # –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ö—ç–≤–ª—ç–Ω—ç
            # if verbose:
            #     print(f"[HAM] {filename}: {prediction}")

        # Spam ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä —Ç—É—Ä—à–∏–Ω–∞
        spam_folder = os.path.join(dev_folder_path, "spam")
        for filename in os.listdir(spam_folder):
            if not filename.endswith(".txt"):
                continue
            file_path = os.path.join(spam_folder, filename)
            prediction = self.predict(file_path)
            if prediction == "spam":
                correct += 1
                correct_spam += 1
            total_spam += 1
            total += 1
            # –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ö—ç–≤–ª—ç–Ω—ç
            # if verbose:
            #     print(f"[SPAM] {filename}: {prediction}")

        # –•—ç—Ä –æ–Ω–æ–≤—á—Ç–æ–π —Ö–∞—Ä–∏—É–ª—Å–Ω—ã–≥ —Ç–æ–æ—Ü–æ–æ–ª–Ω–æ
        accuracy = correct / total if total > 0 else 0
        print(f"\nüéØ Accuracy: {accuracy * 100:.2f}% "
              f"({correct}/{total})")
        error_rate = (total - correct) / total if total > 0 else 0
        print(f"üéØ Error rate: {error_rate * 100:.2f}% "
              f"({total - correct}/{total})")

        print(f"\nConfusion matrix:")
        print(f"                    Classified as:  ")
        print(f"                 |    0   |    1    ")
        print(f"Correct label: 0 |  {correct_spam}   |   {total_spam - correct_spam}")
        print(f"Correct label: 1 |   {total_ham - correct_ham}   |  {correct_ham}")

        precision = correct_ham / (correct_ham + total_spam - correct_spam)
        recall = correct_ham / total_ham
        f1 = 2* (precision*recall)/(precision+recall)

        print(f"False alarm rate: {(total_spam - correct_spam)*100/total_spam:.2f}%")
        print(f"Missed detection rate: {(total_ham - correct_ham)*100/total_ham:.2f}%")
        print(f"Precision: { precision* 100:.2f}%")
        print(f"Recall: { recall*100:.2f}%")
        print(f"F1 score: {f1*100:.2f}%")


        return accuracy
    

    # “Æ—Ä –¥“Ø–Ω–≥ —Ö—ç–≤–ª—ç–Ω—ç
    def printResult(self):
        print(f"–ú–æ–¥–µ–ª—ã–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω:")
        print(f"üìö –Ø–ª–≥–∞–∞—Ç–∞–π “Ø–≥–∏–π–Ω —Ç–æ–æ: {self.data['vocab_size']}")
        print(f"üì® –ù–∏–π—Ç ham “Ø–≥–∏–π–Ω —Ç–æ–æ: {self.data['ham_total']}")
        print(f"üì® –ù–∏–π—Ç spam “Ø–≥–∏–π–Ω —Ç–æ–æ: {self.data['spam_total']}")
        print(f"‚öñÔ∏è  P(ham): {self.data['p_ham']:.3f}, P(spam): {self.data['p_spam']:.3f}")

    # –§–∞–π–ª –±–æ–ª–≥–æ–Ω –¥–∞—Ö—å “Ø–≥–∏–π–Ω —Ç–æ–æ–≥ —Ö—ç–≤–ª—ç–Ω—ç
    def saveDetailed(self, folder_path, output_filename):
        vocab = sorted(list(self.data["vocab"]))
        with open(output_filename, "w", encoding="utf-8") as f:
            # –¢–æ–ª–≥–æ–π —Ö—ç—Å—ç–≥
            f.write("filename," + ",".join(vocab) + "\n")

            # –§–∞–π–ª –±–æ–ª–≥–æ–Ω–æ–æ—Ä –¥–∞–≤—Ç–∞–Ω–∞
            for filename in os.listdir(folder_path):
                if not filename.endswith(".txt"):
                    continue
                file_path = os.path.join(folder_path, filename)
                words = self.getCleanWords(file_path).split()
                counter = Counter(words)

                # –¢—É—Ö–∞–π–Ω —Ñ–∞–π–ª –¥–æ—Ç–æ—Ä—Ö “Ø–≥–∏–π–Ω —Ç–æ–æ–≥ —Ö—ç–≤–ª—ç–Ω—ç
                row_values = [str(counter.get(word, 0)) for word in vocab]
                f.write(filename + "," + ",".join(row_values) + "\n")

        print(f"‚úÖ '{output_filename}'-–≥ —Ö–∞–¥–≥–∞–ª–ª–∞–∞")


# Main
if __name__ == "__main__":
    folder_path_ham = "data/train/ham"
    folder_path_spam = "data/train/spam"
    model = MailClassifierModel(folder_path_ham, folder_path_spam)

    # # –ì–∞–Ω—Ü —Ñ–∞–π–ª –¥—ç—ç—Ä —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–Ω—ç
    # model.predict(r"data/dev/ham/0689.2000-03-22.farmer.ham.txt")
    # model.predict(r"data/dev/spam/0006.2003-12-18.GP.spam.txt")

    # –ù–∏–π—Ç —Ñ–∞–π–ª –¥—ç—ç—Ä —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–Ω—ç
    model.evaluate("data/dev")

    model.saveDetailed(folder_path_ham, "details_ham.txt")
    model.saveDetailed(folder_path_spam, "details_spam.txt")

    # precision ‚úÖ
    # accuracy  ‚úÖ
    # recall
    # confusion matrix
